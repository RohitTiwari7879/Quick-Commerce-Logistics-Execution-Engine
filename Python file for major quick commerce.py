# -*- coding: utf-8 -*-
"""Executive engine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v2x0M5vuvh276AY56swvpf3HYNUwa22x
"""

# import libraries
!pip install faker
import pandas as pd
import numpy as np
from faker import Faker
import random
from textblob import TextBlob # Automated Sentiment Analysis using Natural Language Processsing

# Initialize Faker for Indian context
fake = Faker('en_IN')
num_rows = 10000

# 1. Define Parameters
areas_clean = ["Sector 45", "Cyber Hub", "Sohna Road", "DLF Phase 3", "Golf Course Road"]
# Dictionary to simulate "Dirty Data" (Typos and variations)
area_typos = {
    "Sector 45": ["Sec-45", "S. 45", "Sector45", "GGN Sec 45"],
    "Cyber Hub": ["Cyberhub", "Cyber-Hub", "Cyb Hub", "Cyber Hub GGN"],
    "Sohna Road": ["Sohna-Rd", "Sohna Rd.", "SohnaRoad", "Shna Rd"],
    "DLF Phase 3": ["DLF Ph 3", "DLF-3", "DLF Phase3", "Phase 3 DLF"],
    "Golf Course Road": ["GCR", "Golf Course Rd", "GolfCourse Rd", "G.C. Road"]
}

delivery_status_list = ["Delivered", "Cancelled", "Refunded", "Returned"]

# Master list of reviews for Word Cloud and Sentiment analysis
all_reviews = [
    "Excellent service, the delivery was incredibly fast and items were fresh.",
    "Very impressed with the packaging and the delivery partner was very polite.",
    "Super fast delivery! Got my groceries in 8 minutes, great job Team.",
    "Good quality products and very reliable service for daily needs.",
    "Amazing experience, everything was perfectly packed and delivered on time.",
    "Very bad experience, the milk was leaking and delivery was delayed by 20 minutes.",
    "Missing items in my order and the support team is not responding properly.",
    "Poor quality of vegetables, they looked stale and old. Not recommended.",
    "Late delivery again. Sector 45 dark store always has issues with speed.",
    "Received wrong items, very frustrating experience with the return process.",
    "Average experience. Delivery was on time but packaging could be better.",
    "Decent service, but some items were near expiry. Okay for emergency.",
    "The app is good but the delivery took exactly 15 minutes, not 10.",
    "Standard experience, nothing special but got what I ordered.",
    "Items were fine but the delivery partner couldn't find my location easily."
]

data = []

for i in range(num_rows):
    # Simulate Dirty Area names (70% probability of a typo)
    actual_area = random.choice(areas_clean)
    display_area = actual_area
    if random.random() < 0.7:
        display_area = random.choice(area_typos[actual_area])

    # 3-Month date range for trend analysis
    order_date = fake.date_between(start_date='-90d', end_date='today')
    if random.random() < 0.1:
        order_date = order_date.strftime('%d/%m/%Y')
    else:
        order_date = order_date.strftime('%Y-%m-%d')

    # 1. Select a random review
    review_text = random.choice(all_reviews)

    # 2. Automated NLP Sentiment Analysis
    analysis = TextBlob(review_text)
    polarity_score = analysis.sentiment.polarity # Ranges from -1 (Negative) to 1 (Positive)

    # Bucket the polarity into categories for easy filtering
    if polarity_score > 0.1:
        detected_sentiment = "Positive"
    elif polarity_score < -0.1:
        detected_sentiment = "Negative"
    else:
        detected_sentiment = "Neutral"


    # ISSUE 3: Logic Conflict (Prep + Transit != Actual)
    prep = random.randint(2, 10)
    transit = random.randint(5, 20)
    # 20% of the time, the sum won't match the total time
    if random.random() < 0.2:
        actual_total = prep + transit + random.randint(5, 15)
    else:
        actual_total = prep + transit

    data.append({
        "OrderID": f"ORD{100000 + i}",
        "CustomerID": f"CUST{random.randint(1, 2000)}",
        "OrderDate": order_date,
        "Area": display_area,
        "ActualTime": actual_total,
        "Sentiment": detected_sentiment,
        "Sentiment_Score": polarity_score, # Capture the raw TextBlob value
        "Review": review_text,
        "Total_Amount": round(random.uniform(100, 2500), 2),
        "Delivery_Status": random.choice(delivery_status_list),
        "Prep_Time": prep,
        "Transit_Time": transit
    })

df = pd.DataFrame(data)

# Inject Data Type Errors into 'ActualTime' (e.g., adding "mins" string)
df['ActualTime'] = df['ActualTime'].astype(object)
df.loc[df.sample(frac=0.1).index, 'ActualTime'] = df['ActualTime'].astype(str) + " mins"


# ISSUE 4: Missing Values (NULLS/NaNs)
# Dropping 5% of CustomerIDs and 3% of Total_Amounts
df.loc[df.sample(frac=0.05).index, 'CustomerID'] = np.nan
df.loc[df.sample(frac=0.03).index, 'Total_Amount'] = np.nan

# ISSUE 5: Outliers (Extreme Values)
# Injecting 500-minute delivery times (impossible for Quick Commerce)
df.loc[df.sample(n=20).index, 'ActualTime'] = 500

# ISSUE 6: Duplicates
# Copying 50 rows and appending them again
duplicates = df.sample(n=50)
df = pd.concat([df, duplicates], ignore_index=True)

# ISSUE 7: Data Type Pollution (Mixed Strings/Numbers)
df['ActualTime'] = df['ActualTime'].astype(object)
df.loc[df.sample(frac=0.05).index, 'ActualTime'] = "Delayed"

# Save dataset to Excel
df.to_excel('QuickCommerce_Major_v2_NLP.xlsx', index=False)
print("Dataset Ready: 10,000 rows with raw Sentiment Scores generated!")